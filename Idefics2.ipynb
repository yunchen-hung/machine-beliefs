{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7dd2944b28eb4233ab49460fded363e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a647ecad5b664eed9e184e60aadbde66",
              "IPY_MODEL_79d840a76261420c935230051ae8669b",
              "IPY_MODEL_46cdfde6205b4c77b8d8480ba77a7917"
            ],
            "layout": "IPY_MODEL_a6e1e7a1a0f14c67b516d12d88908e61"
          }
        },
        "a647ecad5b664eed9e184e60aadbde66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74b4c10c0f124f63b7b598b7b4972d44",
            "placeholder": "​",
            "style": "IPY_MODEL_3ec1e1f7882942858817eb52e555e6f5",
            "value": "generation_config.json: 100%"
          }
        },
        "79d840a76261420c935230051ae8669b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c714ceb23734fd7a24a799ab538770e",
            "max": 185,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d4dd887164454c56a572e038dfe631b5",
            "value": 185
          }
        },
        "46cdfde6205b4c77b8d8480ba77a7917": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_759c2a6f63214c75a1836d996607760d",
            "placeholder": "​",
            "style": "IPY_MODEL_18c744d19e7540e29acfe65b716b7587",
            "value": " 185/185 [00:00&lt;00:00, 19.0kB/s]"
          }
        },
        "a6e1e7a1a0f14c67b516d12d88908e61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74b4c10c0f124f63b7b598b7b4972d44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ec1e1f7882942858817eb52e555e6f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c714ceb23734fd7a24a799ab538770e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4dd887164454c56a572e038dfe631b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "759c2a6f63214c75a1836d996607760d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18c744d19e7540e29acfe65b716b7587": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HsyBrY-IJg4I",
        "outputId": "6f08a843-d376-4087-c63e-ced7b08d98ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bitsandbytes is already installed.\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Cell 1: Setup and Installations\n",
        "# =========================\n",
        "import os\n",
        "\n",
        "try:\n",
        "    import bitsandbytes\n",
        "    print(\"bitsandbytes is already installed.\")\n",
        "except ImportError:\n",
        "    print(\"Installing bitsandbytes, accelerate, and dependencies...\")\n",
        "    !pip install -q -U bitsandbytes accelerate transformers kagglehub \"pillow<12.0\"\n",
        "    print(\"Installation complete. If you see CUDA / bitsandbytes errors, restart the runtime.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Cell 2: Imports & Configuration\n",
        "# =========================\n",
        "import torch\n",
        "from transformers import AutoProcessor, AutoModelForVision2Seq, BitsAndBytesConfig\n",
        "from transformers.image_utils import load_image\n",
        "import kagglehub\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# Set device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Running on device: {device}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYWpw0p4KOeP",
        "outputId": "52a4e2aa-ee64-4169-d237-99d526a40ecb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Cell 3: Data Loading (FG-NET)\n",
        "# =========================\n",
        "print(\"Downloading/Loading FG-NET dataset...\")\n",
        "path = kagglehub.dataset_download(\"aiolapo/fgnet-dataset\")\n",
        "\n",
        "def get_image_path(pattern):\n",
        "    matches = glob.glob(pattern, recursive=True)\n",
        "    if not matches:\n",
        "        raise FileNotFoundError(f\"No match found for pattern: {pattern}\")\n",
        "    return matches[0]\n",
        "\n",
        "age_paths = {\n",
        "    'newborn':          get_image_path(f\"{path}/**/080A00.JPG\"),\n",
        "    'older_infant':     get_image_path(f\"{path}/**/080A01.JPG\"),\n",
        "    'toddler':          get_image_path(f\"{path}/**/080A02.JPG\"),\n",
        "    'preschool_child':  get_image_path(f\"{path}/**/080A04.JPG\"),\n",
        "    'schoolage_child':  get_image_path(f\"{path}/**/080A07.JPG\"),\n",
        "}\n",
        "\n",
        "print(\"Sample image paths:\")\n",
        "for k, v in age_paths.items():\n",
        "    print(f\"  {k}: {v}\")\n"
      ],
      "metadata": {
        "id": "yybfa9-PKPOJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecd9cadd-a718-4043-ffcf-39cbb7958ab5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading/Loading FG-NET dataset...\n",
            "Using Colab cache for faster access to the 'fgnet-dataset' dataset.\n",
            "Sample image paths:\n",
            "  newborn: /kaggle/input/fgnet-dataset/FGNET/images/080A00.JPG\n",
            "  older_infant: /kaggle/input/fgnet-dataset/FGNET/images/080A01.JPG\n",
            "  toddler: /kaggle/input/fgnet-dataset/FGNET/images/080A02.JPG\n",
            "  preschool_child: /kaggle/input/fgnet-dataset/FGNET/images/080A04.JPG\n",
            "  schoolage_child: /kaggle/input/fgnet-dataset/FGNET/images/080A07.JPG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Cell 4: Define Questions\n",
        "# =========================\n",
        "questions_on_empiricism = [\n",
        "    'Alex can see things with his eyes. When could Alex see with his eyes for the first time?',\n",
        "    'When there is a sound close by, Alex can hear it. When could Alex hear sounds for the first time?',\n",
        "    'When seeing a red flower and a blue flower, Alex can tell that they are different colors. Alex can tell colors apart. When could Alex tell colors apart for the first time?',\n",
        "    'When there is a car approaching, Alex can tell that the car is getting closer. Alex can tell what is near and what is far. When could Alex tell near and far for the first time?',\n",
        "    'When Alex sees someone hold an object and then drop it, Alex thinks the object will fall. Alex thinks objects will fall if we let go of them. When could Alex think that for the first time?',\n",
        "    'If Alex sees a toy being hidden in a box, he will think the object is still there even though he can no longer see it. When could Alex think that for the first time?',\n",
        "    'If Alex sees two cookies, one with 5 chocolate chips in it and one with 20 chocolate chips in it, he can tell which cookie has more chocolate chips without counting. When could Alex tell which has more for the first time?',\n",
        "    'If Alex sees a turtle that is upside down and struggling to get on its feet, he thinks that he should help the turtle. Alex thinks that helping is the right thing to do. When could Alex think that for the first time?',\n",
        "    'Alex can read books. When could Alex read for the first time?'\n",
        "]\n",
        "print(f\"Loaded {len(questions_on_empiricism)} questions.\")\n"
      ],
      "metadata": {
        "id": "Ds1qYooGKSQb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b93fa0b-2f5e-47d6-e140-3cb87b615708"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 9 questions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Cell 5: Model Initialization (Idefics2-8B)\n",
        "# =========================\n",
        "print(\"Loading Idefics2 model...\")\n",
        "model_id = \"HuggingFaceM4/idefics2-8b\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "# 4-bit quantization to fit into consumer GPUs (e.g., Colab T4)\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "# Attention implementation: flash_attention_2 only on newer GPUs (A100/H100)\n",
        "if torch.cuda.is_available():\n",
        "    major_cc, _ = torch.cuda.get_device_capability()\n",
        "    attn_impl = \"flash_attention_2\" if major_cc >= 8 else \"eager\"\n",
        "else:\n",
        "    attn_impl = \"eager\"\n",
        "\n",
        "print(f\"Using attention implementation: {attn_impl}\")\n",
        "\n",
        "model = AutoModelForVision2Seq.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=quantization_config,\n",
        "    _attn_implementation=attn_impl,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",  # let Accelerate place the model\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "print(\"Model loaded successfully.\")\n"
      ],
      "metadata": {
        "id": "a6-l_ek3KX4H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "134d028514ae4e1b873d3c83b9b0c152",
            "7dd2944b28eb4233ab49460fded363e0",
            "a647ecad5b664eed9e184e60aadbde66",
            "79d840a76261420c935230051ae8669b",
            "46cdfde6205b4c77b8d8480ba77a7917",
            "a6e1e7a1a0f14c67b516d12d88908e61",
            "74b4c10c0f124f63b7b598b7b4972d44",
            "3ec1e1f7882942858817eb52e555e6f5",
            "6c714ceb23734fd7a24a799ab538770e",
            "d4dd887164454c56a572e038dfe631b5",
            "759c2a6f63214c75a1836d996607760d",
            "18c744d19e7540e29acfe65b716b7587"
          ]
        },
        "outputId": "312e8c31-e37a-4832-f151-8f9008a39a16"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "134d028514ae4e1b873d3c83b9b0c152"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7dd2944b28eb4233ab49460fded363e0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Cell 6: The Experiment Loop\n",
        "# =========================\n",
        "print(f\"Starting Inference on {len(questions_on_empiricism)} questions...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load images as PIL objects\n",
        "image1 = load_image(age_paths['newborn'])\n",
        "image2 = load_image(age_paths['toddler'])\n",
        "image3 = load_image(age_paths['schoolage_child'])\n",
        "\n",
        "results = []\n",
        "\n",
        "def extract_image_choice(response: str):\n",
        "    \"\"\"\n",
        "    Heuristic to extract 'Image 1', 'Image 2', or 'Image 3' from Idefics2's text.\n",
        "    \"\"\"\n",
        "    for choice in [\"Image 1\", \"Image 2\", \"Image 3\"]:\n",
        "        if choice.lower() in response.lower():\n",
        "            return choice\n",
        "    # Fallback: just return last sentence-ish\n",
        "    return response.strip().split(\"\\n\")[-1][:100]\n",
        "\n",
        "for i, question in enumerate(questions_on_empiricism, start=1):\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\",  \"text\": \"User:\"},\n",
        "                {\"type\": \"image\"},\n",
        "                {\"type\": \"text\",  \"text\": \"Image 1 is a newborn.\"},\n",
        "                {\"type\": \"image\"},\n",
        "                {\"type\": \"text\",  \"text\": \"Image 2 is a toddler.\"},\n",
        "                {\"type\": \"image\"},\n",
        "                {\"type\": \"text\",  \"text\": \"Image 3 is a school-age child.\"},\n",
        "                {\n",
        "                    \"type\": \"text\",\n",
        "                    \"text\": (\n",
        "                        f\"\\n{question}\\n\"\n",
        "                        \"Which image best answers the question? \"\n",
        "                        \"Reply with 'Image 1', 'Image 2', or 'Image 3'.\"\n",
        "                    ),\n",
        "                },\n",
        "            ],\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Apply chat template\n",
        "    text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "\n",
        "    # IMPORTANT for Idefics2: list of images per example\n",
        "    inputs = processor(\n",
        "        text=[text],\n",
        "        images=[[image1, image2, image3]],  # nested list: batch of size 1, 3 images\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "    )\n",
        "\n",
        "    # Move tensors to GPU if needed\n",
        "    if device == \"cuda\":\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=32,\n",
        "            do_sample=False,\n",
        "        )\n",
        "\n",
        "    generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "    full_response = generated_texts[0]\n",
        "\n",
        "    # Split off the assistant part if template kept \"Assistant:\"\n",
        "    if \"Assistant:\" in full_response:\n",
        "        assistant_part = full_response.split(\"Assistant:\", maxsplit=1)[-1].strip()\n",
        "    else:\n",
        "        assistant_part = full_response.strip()\n",
        "\n",
        "    answer = extract_image_choice(assistant_part)\n",
        "    results.append(answer)\n",
        "\n",
        "    print(f\"Q{i}: {question}\")\n",
        "    print(f\"Idefics2 raw response: {assistant_part}\")\n",
        "    print(f\"Idefics2 parsed answer: {answer}\")\n",
        "    print(\"-\" * 60)\n"
      ],
      "metadata": {
        "id": "ws_AjTNdKbOb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c42e1de-4240-484e-9e20-61cd24193937"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Inference on 9 questions...\n",
            "============================================================\n",
            "Q1: Alex can see things with his eyes. When could Alex see with his eyes for the first time?\n",
            "Idefics2 raw response: Image 1.\n",
            "Idefics2 parsed answer: Image 1\n",
            "------------------------------------------------------------\n",
            "Q2: When there is a sound close by, Alex can hear it. When could Alex hear sounds for the first time?\n",
            "Idefics2 raw response: Image 3.\n",
            "Idefics2 parsed answer: Image 3\n",
            "------------------------------------------------------------\n",
            "Q3: When seeing a red flower and a blue flower, Alex can tell that they are different colors. Alex can tell colors apart. When could Alex tell colors apart for the first time?\n",
            "Idefics2 raw response: Image 3.\n",
            "Idefics2 parsed answer: Image 3\n",
            "------------------------------------------------------------\n",
            "Q4: When there is a car approaching, Alex can tell that the car is getting closer. Alex can tell what is near and what is far. When could Alex tell near and far for the first time?\n",
            "Idefics2 raw response: Image 2.\n",
            "Idefics2 parsed answer: Image 2\n",
            "------------------------------------------------------------\n",
            "Q5: When Alex sees someone hold an object and then drop it, Alex thinks the object will fall. Alex thinks objects will fall if we let go of them. When could Alex think that for the first time?\n",
            "Idefics2 raw response: Image 1.\n",
            "Idefics2 parsed answer: Image 1\n",
            "------------------------------------------------------------\n",
            "Q6: If Alex sees a toy being hidden in a box, he will think the object is still there even though he can no longer see it. When could Alex think that for the first time?\n",
            "Idefics2 raw response: Image 1.\n",
            "Idefics2 parsed answer: Image 1\n",
            "------------------------------------------------------------\n",
            "Q7: If Alex sees two cookies, one with 5 chocolate chips in it and one with 20 chocolate chips in it, he can tell which cookie has more chocolate chips without counting. When could Alex tell which has more for the first time?\n",
            "Idefics2 raw response: Image 3.\n",
            "Idefics2 parsed answer: Image 3\n",
            "------------------------------------------------------------\n",
            "Q8: If Alex sees a turtle that is upside down and struggling to get on its feet, he thinks that he should help the turtle. Alex thinks that helping is the right thing to do. When could Alex think that for the first time?\n",
            "Idefics2 raw response: Image 3.\n",
            "Idefics2 parsed answer: Image 3\n",
            "------------------------------------------------------------\n",
            "Q9: Alex can read books. When could Alex read for the first time?\n",
            "Idefics2 raw response: Image 3.\n",
            "Idefics2 parsed answer: Image 3\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Cell 7: Save/Export Data\n",
        "# =========================\n",
        "print(\"Raw Results for Plotting / Analysis:\")\n",
        "print(results)\n"
      ],
      "metadata": {
        "id": "03ELH52sKew_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc5a5e0a-7802-4eac-c2e1-42f831a16592"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Results for Plotting / Analysis:\n",
            "['Image 1', 'Image 3', 'Image 3', 'Image 2', 'Image 1', 'Image 1', 'Image 3', 'Image 3', 'Image 3']\n"
          ]
        }
      ]
    }
  ]
}