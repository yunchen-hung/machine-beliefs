{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c45fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_on_empiricism = [\n",
    "    'Alex can see things with his eyes. When could Alex see with his eyes for the first time?',\n",
    "    'When there is a sound close by, Alex can hear it. When could Alex hear sounds for the first time?',\n",
    "    'When seeing a red flower and a blue flower, Alex can tell that they are different colors. Alex can tell colors apart. \\\n",
    "    When could Alex tell colors apart for the first time?',\n",
    "    'When there is a car approaching, Alex can tell that the car is getting closer. Alex can tell what is near and what is far. \\\n",
    "    When could Alex tell near and far for the first time?',\n",
    "    'When Alex sees someone hold an object and then drop it, Alex thinks the object will fall. Alex thinks objects will fall if we let go of them.\\\n",
    "     When could Alex think that for the first time?',\n",
    "    'If Alex sees a toy being hidden in a box, he will think the object is still there even though he can no longer see it.\\\n",
    "     When could Alex think that for the first time?',\n",
    "    'If Alex sees two cookies, one with 5 chocolate chips in it and one with 20 chocolate chips in it, he can tell which cookie has more chocolate chips without counting. \\\n",
    "     When could Alex tell which has more for the first time?',\n",
    "    'If Alex sees a turtle that is upside down and struggling to get on its feet, he thinks that he should help the turtle. Alex thinks that helping is the right thing to do. \\\n",
    "    When could Alex think that for the first time?',\n",
    "    'Alex can read books. When could Alex read for the first time? '\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b288c5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import glob\n",
    "\n",
    "# FGNET age progression dataset from kaggle\n",
    "path = kagglehub.dataset_download(\"aiolapo/fgnet-dataset\")\n",
    "\n",
    "age_paths = {'newborn': glob.glob(f\"{path}/**/080A00.JPG\", recursive=True)[0], \"older_infant\": glob.glob(f\"{path}/**/080A01.JPG\", recursive=True)[0],\n",
    "            'toddler': glob.glob(f\"{path}/**/080A02.JPG\", recursive=True)[0], 'preschool_child': glob.glob(f\"{path}/**/080A04.JPG\", recursive=True)[0],\n",
    "            'schoolage_child': glob.glob(f\"{path}/**/080A07.JPG\", recursive=True)[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aab1078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, Gemma3ForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "#template from https://huggingface.co/google/gemma-3-27b-it\n",
    "model_id = \"google/gemma-3-27b-it\"\n",
    "\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    model_id, device_map=\"auto\"\n",
    ").eval()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"You are a human answering questions for a psychology survey.\"}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"url\": age_paths['newborn']},\n",
    "          {\"type\": \"image\", \"url\": age_paths['toddler']},\n",
    "          {\"type\": \"image\", \"url\": age_paths['schoolage_child']},\n",
    "          {\"type\": \"text\", \"text\": f'{questions_on_empiricism[0]} Pick from image 1, 2, 3. \\\n",
    "           You must reply with either 1, 2, or 3 and specify the age of the child.'},\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages, add_generation_prompt=True, tokenize=True,\n",
    "    return_dict=True, return_tensors=\"pt\"\n",
    ").to(model.device, dtype=torch.bfloat16)\n",
    "\n",
    "input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    generation = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
    "    generation = generation[0][input_len:]\n",
    "\n",
    "decoded = processor.decode(generation, skip_special_tokens=True)\n",
    "print(decoded)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
